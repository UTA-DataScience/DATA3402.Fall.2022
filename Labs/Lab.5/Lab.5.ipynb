{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5\n",
    "\n",
    "This lab will take 2 weeks. You are to complete Exercises 1-4 in 1 week and Exercises 5-8 by the following week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Reading\n",
    "\n",
    "### HiggsML\n",
    "In 2014, some of my colleagues from the ATLAS experiment put together a Higgs Machine Learning Challenge, which was hosted on [Kaggle](https://www.kaggle.com). Please read sections 1 and 3 (skip/skim 2) of [The HiggsML Technical Documentation](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf). \n",
    "\n",
    "Kaggle is a platform for data science competitions, with cash awards for winners. Kaggle currently hosts over 50,000 public datasets and associated competitions. Later in the course we will look at a variety of problems hosted on Kaggle and similar platforms. \n",
    "\n",
    "### SUSY Dataset\n",
    "\n",
    "For the next few labs we will use datasets used in the [first paper on Deep Learning in High Energy physics](https://arxiv.org/pdf/1402.4735.pdf). Please read up to the \"Deep Learning\" section (end of page 5). This paper demonstrates that Deep Neural Networks can learn from raw data the features that are typically used by physicists for searches for exotics particles. The authors provide the data they used for this paper. They considered two benchmark scenarios: Higgs and SUSY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Download SUSY Dataset\n",
    "\n",
    "The information about the dataset can be found at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We'll start with the [SUSY Dataset](https://archive.ics.uci.edu/ml/datasets/SUSY). \n",
    "\n",
    "### Download\n",
    "In a terminal, download the data directly from the source and then decompress it. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To download:\n",
    "    * On Mac OS: \n",
    "    `curl http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz > SUSY.csv.gz`\n",
    "\n",
    "    * In linux:\n",
    "    `wget http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz`\n",
    "\n",
    "* To uncompress:\n",
    "`gunzip SUSY.csv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  879M  100  879M    0     0  12.0M      0  0:01:12  0:01:12 --:--:-- 9102k0:00:46  0:00:28 13.0M\n"
     ]
    }
   ],
   "source": [
    "!curl http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz > SUSY.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip SUSY.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4691400\r\n",
      "-rw-r--r--@ 1 afarbin  staff    15K Mar 25 09:59 Lab.5.ipynb\r\n",
      "-rw-r--r--@ 1 afarbin  staff   6.7M Mar 25 10:56 Lab.5.pdf\r\n",
      "-rw-r--r--@ 1 afarbin  staff   2.2G Mar 25 09:58 SUSY.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided as a comma separated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000000000000000e+00,9.728614687919616699e-01,6.538545489311218262e-01,1.176224589347839355e+00,1.157156467437744141e+00,-1.739873170852661133e+00,-8.743090629577636719e-01,5.677649974822998047e-01,-1.750000417232513428e-01,8.100607395172119141e-01,-2.525521218776702881e-01,1.921887040138244629e+00,8.896374106407165527e-01,4.107718467712402344e-01,1.145620822906494141e+00,1.932632088661193848e+00,9.944640994071960449e-01,1.367815494537353516e+00,4.071449860930442810e-02\r\n",
      "1.000000000000000000e+00,1.667973041534423828e+00,6.419061869382858276e-02,-1.225171446800231934e+00,5.061022043228149414e-01,-3.389389812946319580e-01,1.672542810440063477e+00,3.475464344024658203e+00,-1.219136357307434082e+00,1.295456290245056152e-02,3.775173664093017578e+00,1.045977115631103516e+00,5.680512785911560059e-01,4.819284379482269287e-01,0.000000000000000000e+00,4.484102725982666016e-01,2.053557634353637695e-01,1.321893453598022461e+00,3.775840103626251221e-01\r\n",
      "1.000000000000000000e+00,4.448399245738983154e-01,-1.342980116605758667e-01,-7.099716067314147949e-01,4.517189264297485352e-01,-1.613871216773986816e+00,-7.686609029769897461e-01,1.219918131828308105e+00,5.040258169174194336e-01,1.831247568130493164e+00,-4.313853085041046143e-01,5.262832045555114746e-01,9.415140151977539062e-01,1.587535023689270020e+00,2.024308204650878906e+00,6.034975647926330566e-01,1.562373995780944824e+00,1.135454416275024414e+00,1.809100061655044556e-01\r\n",
      "1.000000000000000000e+00,3.812560737133026123e-01,-9.761453866958618164e-01,6.931523084640502930e-01,4.489588439464569092e-01,8.917528986930847168e-01,-6.773284673690795898e-01,2.033060073852539062e+00,1.533040523529052734e+00,3.046259880065917969e+00,-1.005284786224365234e+00,5.693860650062561035e-01,1.015211343765258789e+00,1.582216739654541016e+00,1.551914215087890625e+00,7.612152099609375000e-01,1.715463757514953613e+00,1.492256760597229004e+00,9.071890264749526978e-02\r\n",
      "1.000000000000000000e+00,1.309996485710144043e+00,-6.900894641876220703e-01,-6.762592792510986328e-01,1.589282631874084473e+00,-6.933256387710571289e-01,6.229069828987121582e-01,1.087561845779418945e+00,-3.817416727542877197e-01,5.892043709754943848e-01,1.365478992462158203e+00,1.179295063018798828e+00,9.682182073593139648e-01,7.285631299018859863e-01,0.000000000000000000e+00,1.083157896995544434e+00,4.342924803495407104e-02,1.154853701591491699e+00,9.485860168933868408e-02\r\n"
     ]
    }
   ],
   "source": [
    "filename=\"SUSY.csv\"\n",
    "# print out the first 5 lines using unix head command\n",
    "!head -5  \"SUSY.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Look\n",
    "\n",
    "Each row represents a LHC collision event. Each column contains some observable from that event. The variable names are ([based on documentation](https://archive.ics.uci.edu/ml/datasets/SUSY)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarNames=[\"signal\", \"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these variables represent the \"raw\" kinematics of the observed final state particles, while others are \"features\" that are derived from these raw quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawNames=[\"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\"]\n",
    "FeatureNames=list(set(VarNames[1:]).difference(RawNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l_1_pT',\n",
       " 'l_1_eta',\n",
       " 'l_1_phi',\n",
       " 'l_2_pT',\n",
       " 'l_2_eta',\n",
       " 'l_2_phi',\n",
       " 'MET',\n",
       " 'MET_phi']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RawNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MT2',\n",
       " 'S_R',\n",
       " 'MET_rel',\n",
       " 'M_Delta_R',\n",
       " 'M_TR_2',\n",
       " 'R',\n",
       " 'dPhi_r_b',\n",
       " 'axial_MET',\n",
       " 'cos_theta_r1',\n",
       " 'M_R']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeatureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pandas to read in the file, and matplotlib to make plots. The following ensures pandas is installed and sets everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename, dtype='float64', names=VarNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the data in Jupyter by just evaluateing the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column stores the \"truth\" label of whether an event was signal or not. Pandas makes it easy to create dataframes that store only the signal or background events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig=df[df.signal==1]\n",
    "df_bkg=df[df.signal==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example plots the signal and background distributions of every variable. Note that we use VarNames[1:] to skip the first variable, which was the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for var in VarNames[1:]:\n",
    "    print (var)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(np.array(df_sig[var]),bins=100,histtype=\"step\", color=\"red\",label=\"signal\",normed=True, stacked=True)\n",
    "    plt.hist(np.array(df_bkg[var]),bins=100,histtype=\"step\", color=\"blue\", label=\"background\",normed=True, stacked=True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Make nice figures\n",
    "\n",
    "Now use `matplotlib` to reproduce as closely as you can figures 5 and 6 from the paper. This exercise is intended to get you to familiarize yourself with making nicely formatted `matplotlib` figures with multiple plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Correlation\n",
    "\n",
    "### Exercise 4.1\n",
    "\n",
    "#### Part a\n",
    "Write a function that creates pair plots and use it to compare variables in the SUSY and Higgs samples, separately for low and high-level features. Refer to [Data 1401 Visualization Lecture](https://github.com/UTA-DataScience/DATA1401.2020.Fall/blob/master/Lectures/Lecture.36/Lecture.36.ipynb) for details. Do not use `seaborn`.\n",
    "\n",
    "#### Part b\n",
    "Making these plots can be slow because creating each plot initiates a full loop over the data. Make at least one modification to your function in part a to speed it up. Can you propose a different method of creating histograms that would speed up making such pair plots?\n",
    "\n",
    "#### Part c\n",
    "Which observables appear to be best for separating signal from background?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "#### Part a\n",
    "Install [tabulate](https://github.com/astanin/python-tabulate). \n",
    "\n",
    "#### Part b\n",
    "Use numpy to compute the [covariance matrix](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) and [correlation matrix](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html) between all observabes, and separately between low and high-level features.\n",
    "\n",
    "#### Part c\n",
    "Use tabulate to create a well formatted table of the covariance and correlation matrices, with nice headings and appropriate significant figures. Embed the table into this notebook.\n",
    "\n",
    "#### Part d\n",
    "Write a function that takes a dataset and appropriate arguments and performs steps b and c.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Example code for embedding a `tabulate` table into a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"A\",1,2],\n",
    "        [\"C\",3,4],\n",
    "        [\"D\",5,6]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"X\",\"Y\",\"Z\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Selection\n",
    "\n",
    "### Exercise 5.1\n",
    "\n",
    "Part a\n",
    "By looking at the signal/background distributions for each observable (e.g. $x$) determine which selection criteria would be optimal: \n",
    "\n",
    "1. $x > x_c$\n",
    "2. $x < x_c$\n",
    "3. $|x - \\mu| > x_c$\n",
    "4. $|x - \\mu| < x_c$\n",
    "\n",
    "where $x_c$ is value to be determined below.\n",
    "\n",
    "### Exercise 5.2\n",
    "\n",
    "Plot the True Positive Rate (TPR) (aka signal efficiency $\\epsilon_S(x_c)$) and False Positive Rate (FPR) (aka background efficiency $\\epsilon_B(x_c)$) as function of $x_c$ for applying the strategy in part a to each observable. \n",
    "\n",
    "### Exercise 5.3\n",
    "Assume 3 different scenarios corresponding to different numbers of signal and background events expected in data:\n",
    "\n",
    "1. Expect $N_S=10$, $N_B=100$.\n",
    "1. Expect $N_S=100$, $N_B=1000$.\n",
    "1. Expect $N_S=1000$, $N_B=10000$.\n",
    "1. Expect $N_S=10000$, $N_B=100000$.\n",
    "\n",
    "Plot the significance ($\\sigma_{S'}$) for each observable as function of $x_c$ for each scenario, where \n",
    "\n",
    "$\\sigma_{S'}= \\frac{N'_S}{\\sqrt{N'_S+N'_B}}$\n",
    "\n",
    "and $N'_{S,B} = \\epsilon_{S,B}(x_c) * N_{S,B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Cut Flow\n",
    "\n",
    "\n",
    "### Exercise 6.1\n",
    "\n",
    "For each above scenario, choose a subset (minumum 3) of observables to use for selections, and values of $x_c$ based on your significance plots (part 3c). \n",
    "\n",
    "### Exercise 6.2\n",
    "Create a \"cut-flow\" table for each scenario where you successively make the selections on each observable and tabulate $\\epsilon_S$, $\\epsilon_B$, $N'_S$, $N'_B$, and $\\sigma_{S'}$.\n",
    "\n",
    "### Exercise 6.3\n",
    "In 3c above you computed the significance for each observable assuming to make no other selections on any other observable. If the variables are correlated, then this assumption can lead to non-optimial results when selecting on multiple variables. By looking at the correlation matrices and your answers to 4b, identify where this effect could be most detrimental to the significance. Attempt to correct the issue by applying the selection in one observable and then optimizing (part 3c) for a second observable. What happens if you change the order of your selection (make selection on second and optimize on first)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: ROC Curves\n",
    "\n",
    "### Exercise 7.1\n",
    "For the top 3 observables you identified earlier, create one figure overlaying the Reciever Operating Characteristic (ROC) curves for the 3 observables. Compute the area under the curves and report it in the legend of the figure.\n",
    "\n",
    "### Exercise 7.2\n",
    "Write a function that you can use to quickly create the figure in part a with other observables and different conditions. Note that you will likely revise this function as you do the remainder of the lab.\n",
    "\n",
    "### Exercise 7.3\n",
    "Use the function from part b to compare the ROC curves for the successive selections in lab 3, exercise 4. Specifically, plot the ROC curve after each selection.\n",
    "\n",
    "### Exercise 7.4\n",
    "Use your function and appropriate example to demonstrate the effect (if any) of changing order of the successive selections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Linear Discriminant\n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "Using numpy, compute the between-class $\\bf{S}_B$ and within-class $\\bf{S}_W$ covariance matrices defined as:\n",
    "\n",
    "$$\n",
    "\\bf{S}_B = (\\bf{m_2}-\\bf{m_1})(\\bf{m_2}-\\bf{m_1})^T \\\\\n",
    "$$\n",
    "$$\n",
    "\\bf{S}_W = \\sum_{i=1,2} \\sum_{n=1}^{l_i} (\\bf{x}_n^i - \\bf{m}_i) (\\bf{x}_n^i - \\bf{m}_i)^T\n",
    "$$\n",
    "\n",
    "where $\\bf{m_i}$ are the vectors containing the means for category 1 and 2, here defined as signal and background. Here $\\bf{x}_n^i$ is the vector containing the observables for the $n$th example event in category $i$.\n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "Compute the linear coefficients $\\bf{w} = \\bf{S_W}^{-1}(\\bf{m_2}-\\bf{m_1})$. Compare the histogram of the distribution of $F_n^i=\\bf{w}^T\\bf{x}_n^i$ for the two categories.\n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "Draw the ROC curve for $F_n$. \n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "What is the maximal significance you can obtain in the scenarios in exercise 5? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
